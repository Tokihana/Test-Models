## CLS Block有效性

论点：CLS Block在分类任务上，能够获得和ViT Block相近的效果，且能够减少运行开销

方法：使用IR50 + ViT Block的作为Baseline，使用CLS Attn的CLS Block为验证模型。设置block depth为8；优化器为Adam，eps = 1e-8，betas=(0.9, 0.999)，loss为CrossEntropy；在单个3090上运行实验，设置Batch Size = 256；没有使用weight decay。为避免模型对LR敏感，在以10倍步进行初步LR筛选后，对$0.1^i, i \in [4.5, 4.75, 5, 5.25, 5]$内的不同LR分别进行测试，在RAF-DB数据集上运行60epoch，并比较max acc。

ViT Block与CLS Block的mlp ratio都是4，使用同样的Mixup数据增强方法，设置alpha = 1.0。

结果：

在多种LR下运行的结果如下表，使用CLS Block的表现全面优于使用VIT Block，这是个非常有意思的现象，我需要找到合理的角度来解释这个事情。

| LR       | MODEL    | ACC        |
| -------- | -------- | ---------- |
| 3.20E-05 | Baseline | 82.986     |
|          | CLSFER   | **83.638** |
| 1.80E-05 | Baseline | 84.452     |
|          | CLSFER   | **84.778** |
| 1.00E-05 | Baseline | 83.67      |
|          | CLSFER   | **84.387** |
| 5.60E-06 | Baseline | 84.713     |
|          | CLSFER   | **85.039** |
| 3.20E-06 | Baseline | 83.605     |
|          | CLSFER   | **85.398** |

在batch size=256的条件下，测试在同一张3090显卡上的运行速度，首先预热50 batch，然后取30 batch运行结果的平均样本通过数作为模型的运行速度标准。

对于params与FLOPs，使用thop库的profile方法进行计量。

对于运行阶段占用的显存，使用pytorch的max_memory_allocated方法进行测试。

| Model                      | Batch Size | throughput | FLOPs(G)  | Params.(M) | Memory Usage(M) | Memory Usage(G) |
| -------------------------- | ---------- | ---------- | --------- | ---------- | --------------- | --------------- |
| Baseline(IR50 + ViT Block) | 256        | 1381       | 7.594     | **55.954** | 21580           | 21.07           |
| IR50 + CLS  Block          | 256        | **1790**   | **6.385** | 55.955     | **18479**       | 18.05           |



ViT的论文有没有说明CLS Token和feature token具体包含什么信息？有没有说过。

FER领域，表情相关和表情无关信息最早是谁提的。



1. 其他数据集上也出现这种情况，证明结果有效性。
2. 传统ViT会分CLS 和feature，分别代表什么信息。-> ViT最开始的论文，立论基础
3. CLS token反映了哪些信息，feature token反映了哪些信息。-> feature token**对表情识别没有用**，**该怎么证明**。
4. 精度下一步可以做着用。



做哪些实验：

1. baseline&CLS在其他数据集->证明现象是普遍的。
2. 如果证明是普遍现象，想办法证明feature token没有用。