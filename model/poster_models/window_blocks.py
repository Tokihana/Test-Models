import torch
import torch.nn as nn
from torch.nn import functional as F
from timm.models.layers import trunc_normal_

def window_partition(x, window_size, h_w, w_w):
    '''
    split images (B, H, W, C) to windows (B', ws, ws, C)
    Args:
        x (B, H, W, C) : input features
        window_size (scalar) : size of each window
        h_w, w_w (scalar) : num of windows in each axis
    Returns:
        windows
    '''
    B, H, W, C = x.shape
    x = x.view(B, h_w, window_size, w_w, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    # first permute to (B, h_w, w_w, ws, ws, C), then review to (B * h_w * w_w, ws, ws, C)
    return windows

class window(nn.Module):
    '''
    window generate classes
    Args:
        window_size (scalar) : size of a window
        dim (scalar) : dims for normalize x, same as input channels
                       this param is passed because the LayerNorm should be constructed at __init__
                       where the code can not get the dimension of x by calling x.shape
        x (B, C, H, W) : input features form ir50 backbone
    Returns
        x_windows: split windows vector
        shortcuts: shortcuts of normlized x, for feedforward res
    '''
    def __init__(self, window_size, dim):
        super().__init__()
        self.window_size = window_size
        self.norm = nn.LayerNorm(dim)
    def forward(self, x):
        x = x.permute(0, 2, 3, 1)
        B, H, W, C = x.shape
        x = self.norm(x)
        shortcut = x
        h_w = int(torch.div(H, self.window_size).item())
        w_w = int(torch.div(W, self.window_size).item())
        x_windows = window_partition(x, self.window_size, h_w, w_w)
        x_windows = x_windows.view(-1, self.window_size*self.window_size, C)
        return x_windows, shortcut
    
class WindowAttentionGlobal(nn.Module):
    '''
    Args:
        x (B_, N, C) : x_windows generated by window class, B_ = B * h_w * w_w, N = window_size * window_size
        q_g (B, 1, num_heads, N, dim_head) : landmark query 
        
    Returns:
        x (B_, N, C) : attention result
    '''
    def __init__(self, dim, num_heads,
                 window_size, 
                 qkv_bias=True,
                 qk_sclae=None,
                 attn_drop=0.,
                 proj_drop=0.
                 ):
        super().__init__()
        self.window_size = (window_size, window_size)
        self.num_heads = num_heads
        head_dim = torch.div(dim, num_heads, rounding_mode = 'floor')
        self.scale = qk_sclae or head_dim ** -0.5 # \sqrt d by default
        
        # coordinates
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))
        coords_flatten = torch.flatten(coords, 1)
        
        # relative coordinates
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        # each row corresponds to the relative position with a specific origin patch
        # because B must be extracted from \hat B, the relative_coords should be positive
        # by plus M - 1
        relative_coords[:, :, 0] += self.window_size[0] - 1
        relative_coords[:, :, 1] += self.window_size[1] - 1
        # the \hat B is initialized as 1-D array in each feature dimension, length (2M - 1)^2
        # so the relative_coords should be represent from 2-D array to 1-D array index
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_idx = relative_coords.sum(-1)
        
        # relative_position_bias_table, also \hat B, this table is learnable parameter
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1), num_heads))
        self.register_buffer("relative_position_idx", relative_position_idx)
        
        # attention components
        self.qkv = nn.Linear(dim, dim * 2, bias = qkv_bias) # * 2 only because q_global has given
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        self.softmax = nn.Softmax(dim=-1)
        trunc_normal_(self.relative_position_bias_table, std=.02)
        
    def forward(self, x, q_g):
        B_, N, C = x.shape # feature batch
        B = q_g.shape[0] # global landmark batch size
        head_dim = torch.div(C, self.num_heads, rounding_mode='floor')
        B_dim = torch.div(B_, B, rounding_mode='floor') # nums to repeat q_g
        #print(f"B_dim = {B_dim.cpu().numpy()}")
        kv = self.qkv(x).reshape(B_, N, 2, self.num_heads, head_dim).permute(2, 0, 3, 1, 4) 
        # split channels to different head, then reshape to q_g shape
        k, v = kv[0], kv[1]
        q_g = q_g.repeat(1, B_dim, 1, 1, 1) # repeat q_g
        q = q_g.reshape(B_, self.num_heads, N, head_dim)
        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))
        
        relative_position_bias = self.relative_position_bias_table[self.relative_position_idx.view(-1)].view(
            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()
        attn = attn + relative_position_bias.unsqueeze(0)
        attn = self.softmax(attn)
        attn = self.attn_drop(attn)
        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
        

class CrossAttention(nn.Module):
    def __init__(self, dim, num_heads = 8, window_size = 28, qkv_bias = False, attn_drop=0, proj_drop=0):
        super().__init__()
        self.window_size = (window_size, window_size)
        self.num_heads = num_heads
        head_dim = dim// num_heads
        self.scale = head_dim ** -0.5
        
        self.kv = nn.Linear(dim, dim * 2, bias = qkv_bias) # * 2 only because q_c has given
        self.q = nn.Linear(dim, dim, bias = qkv_bias) # q attn map
        
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        self.softmax = nn.Softmax(dim=-1)
        
        # coordinates
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))
        coords_flatten = torch.flatten(coords, 1)
        
        # relative coordinates
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        # each row corresponds to the relative position with a specific origin patch
        # because B must be extracted from \hat B, the relative_coords should be positive
        # by plus M - 1
        relative_coords[:, :, 0] += self.window_size[0] - 1
        relative_coords[:, :, 1] += self.window_size[1] - 1
        # the \hat B is initialized as 1-D array in each feature dimension, length (2M - 1)^2
        # so the relative_coords should be represent from 2-D array to 1-D array index
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_idx = relative_coords.sum(-1)
        
        # relative_position_bias_table, also \hat B, this table is learnable parameter
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1), num_heads))
        self.register_buffer("relative_position_idx", relative_position_idx)
        
    def forward(self, x, q_c):
        B, N, C = x.shape
        head_dim = torch.div(C, self.num_heads, rounding_mode = 'floor')
        
        kv = self.kv(x).reshape(B, N, 2, self.num_heads, head_dim).permute(2, 0, 3, 1, 4) # (kv, B, num_heads, B, head_dim)
        k, v = kv[0], kv[1]
        q = self.q(x).reshape(B, N, 1, self.num_heads, head_dim).permute(2, 0, 3, 1, 4) # (q, B, num_heads, B, head_dim)
        q = q*self.scale
        
        attn = q@k.transpose(-2, -1)
        relative_position_bias = self.relative_position_bias_table[self.relative_position_idx.view(-1)].view(
            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()
        attn = attn + relative_position_bias.unsqueeze(0)
        attn = self.softmax(attn)
        attn = self.attn_drop(attn)
        
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
        